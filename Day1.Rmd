---
title: "NU-IT Research Computing Services: Regression Models with R - Day 1"
subtitle: "By Diego Gomez-Zara"
output: html_notebook
---

In this workshop, we will learn about the R modeling ecosystem to run linear and logistic regression models. We will also cover model diagnostics, variable selection, and generalized linear models (GLM). Our goals are to learn about:

1. Visualizing, summarizing, and standardizing data before modeling
2. Using linear regressions for continuous outcomes
3. Using logistic regressions for binary outcomes
4. Running diagnostics and goodness of fit
5. Running variable selection and model building. 

This workshop aims to teach you how to use R for building linear regressions. It does not intent to provide mathematical explanations or interpretations of the results, since it is not a statistics workshop. However, we will discuss how models are build and diagnosed, and how the model results are organized. If you have specific questions about choosing, evaluating, and interpreting statistics models or tests, we encourage you to consult with a statistic experts. [The Biostatistics Collaboration Center](https://www.feinberg.northwestern.edu/sites/bcc/) is an excellent resource for researchers affiliated with the Feinberg School of Medicine.Other researchers may request a consult for statistics questions and NU-IT Research Computing Services will evaluate their ability to help on a request by request basis. 

We will load the library `dplyr` for data cleaning and transforming. The second library is `Hmisc`, we will use it for correlation analysis. We will use the third library to plot data's correlations. 

```{r}
library(dplyr)
library(Hmisc)
library(PerformanceAnalytics)
```

We will clean the environment and set a random seed, so we can have all the same results in our environments.
```{r}
rm(list = ls())
set.seed(40)
```

## 1. Import Dataset

This is the "Medical Cost Personal Datasets" dataset and it was downloaded from [Kaggle](https://www.kaggle.com/mirichoi0218/insurance). The motivation is to predict the insurance costs based on individuals' demographics and health conditions. The file has the following columns:

* **age**: age of primary beneficiary
* **gender**: insurance contractor gender. Two values: female, male
* **bmi**: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,
objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
* **children**: Number of children covered by health insurance / Number of dependents
* **smoker**: Smoking
* **region**: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
* **charges**: Individual medical costs billed by health insurance

```{r}
data <- read.csv("insurance.csv")
```

Let's start examining the dataset to understand the attributes, their format and scales.
```{r}
data
```

We can observe that some columns are numeric (`age, bmi, children, charges`), and others are categorical (`gender, smoker, region`). R linear regressions support categorical variables, but we need to transform the string variables first to vectors. 
```{r}
data$gender <- as.factor(data$gender)
```

When we have categorical variables, the regression model will compare all the results with respect to a specific category. That category is called *reference level*. By default, R chooses the reference level *alphabetically*, so in the case of `gender`, `female` will be the reference level. Given that it's a binary variable, it will compare `male` individuals' charges with respect to `female` individuals. We can re-order and set `male` as the reference by running the command `relevel` and using the paramater `ref`.
```{r}
data$gender <- relevel(data$gender, ref = "male")
```

### Exercise 1
Transform the variables `smoker` and `region` to factors. For the `smoker` variable, set `No` as the reference level.
```{r}
data$smoker <- as.factor(data$smoker)
data$region <- as.factor(data$region)
data$smoker <- relevel(data$smoker, ref="no")
```

## 2. Analyze the dataset
We start analyzing the dataset that is numeric. 
```{r}
data_numeric <- data %>% dplyr::select(where(is.numeric))
data_numeric
```

We check the correlations of these numeric variables.
```{r}
corr <- cor(data_numeric, use = "complete.obs")
corr
```

The function rcorr() [in Hmisc package] can be used to compute the significance levels for pearson and spearman correlations. It returns both the correlation coefficients and the p-value of the correlation for all possible pairs of columns in the data table.

```{r}
res <- rcorr(as.matrix(data_numeric))
res
```

The output of the function `rcorr()` is a list containing the following elements: 
* `r` : the correlation matrix
* `n` : the matrix of the number of observations used in analyzing each pair of variables
* `P` : the p-values corresponding to the significance levels of correlations.

If you want to extract the p-values or the correlation coefficients from the output, use this command:

```{r}
# Extract the correlation coefficients
res$r
# Extract p-values
res$P
```

Using the package, we can plot the histograms and the relationships among variables using one command

```{r}
chart.Correlation(data_numeric, histogram=TRUE, pch=19)
```

## 3. Scaling the dataset

As we saw, the columns have different scales, minimum, and maximum values. It is a good practice to scale the columns before running a model.

Standardization of datasets is a common requirement for regression models. The goal is to transform each variable to **zero mean and unit variance**. This process **transforms the data to center it by removing the mean value of each feature, then scales it by dividing non-constant features by their standard deviation**. However, this transformation does not change the distribution of the data.

<img alt="Z = \frac{x - \mu}{\sigma}" src="https://www.gstatic.com/education/formulas2/355397047/en/z_score.svg" role="img" data-atf="0" data-frt="0">

where *Z* is the standard score, *x* is the observed value, *u* is the mean of the sample, and is *s* the standard deviation of the sample. 

The function `scale`centers and scales the columns of a numeric dataframe.
```{r}
data <- data %>% mutate_if(is.numeric, scale)
```

We now see how the data is distributed, their means, and standard deviations.
```{r}
summary(data)
```

We check that the standard deviations are equal to the unit (1). 
```{r}
apply(data[sapply(data, is.numeric)],2,sd)
```


## 4. Build a linear model

Once the data have been inspected, cleaned, and standarized, we can start estimating models. The simplest models (but those with the most assumptions) are those for continuous and unbounded outcomes. We use a linear regression model estimated using Ordinary Least Squares (OLS), which in R can be fit with the `lm()` (linear model) function.

To fit a model in R, we first have to convert our theoretical model into a `formula` â€” a symbolic representation of the model in R syntax. We then can use `lm()` to fit this model.
```{r}
model0 <- lm(charges ~ age, data)
```

We can inspect the methods available for an object `lm`.
```{r}
methods(class = class(model0))
```

We can get more information by passing the fitted model object to the `summary()` function, which provides standard errors, test statistics, and p-values for individual coefficients, as well as goodness-of-fit measures for the overall model.
```{r}
summary(model0)
```

Residuals are useful for checking the goodness of fit and diagnose model violations. We will check more about R^2 and residuals this week. 

We can use the `confint()` method to get interval estimates for the coefficients:
```{r}
confint(model0)
```

And we can use the `anova()` method to get an ANOVA-style table of the model
```{r}
anova(model0)
```

And we can predict values by using the `predict` function. We take the `data`'s first observation and check the real value with the predicted.
```{r}
print(data$charges[1])

predict(model0, data[1,], se.fit = TRUE)
```

### Adding more variables
We can continue adding more variables to the model and check whether the new model is significantly better than `model0`.
```{r}
model1 <- lm(charges ~ age + bmi, data)
summary(model1)
```

Compare the models using an F-test with the `anova()` function
```{r}
anova(model0, model1)
```

### Exercise 2
Create four models by adding sequentially the following attributes: `children, gender, smoker, region`.
* `model2`: `model1` and `children`.
* `model3`: `model2` and `gender`.
* `model4`: `model3` and `smoker`.
* `model5`: `model4` and `region`.

For each model, run the functions `summary`,`confint`, and `anova`.

```{r}
model2 <- lm(charges ~ age + bmi + children, data)
confint(model2)
summary(model2)
```

```{r}
model3 <- lm(charges ~ age + bmi + children + gender, data)
confint(model3)
summary(model3)
```

```{r}
model4 <- lm(charges ~ age + bmi + children + gender + smoker, data)
confint(model4)
summary(model4)
```

```{r}
model5 <- lm(charges ~ age + bmi + children + gender + smoker + region, data)
confint(model5)
summary(model5)
```

Finally, compare all the models
```{r}
anova(model0,model1,model2,model3,model4,model5)
```

### References and resources
* [Institute for Quantitative Social Science - Data Science Workshops](https://iqss.github.io/dss-workshops/Rmodels.html)